{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from ddqn_agent import ddqn_agent\n",
    "from banana_env import banana_env\n",
    "from banana_collector import banana_collector\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected brain name= BananaBrain\n",
      "Selected brain= Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n",
      "Number of actions: 4\n",
      "Number of agents: 1\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "env = banana_env(file_name=\"Banana_Windows_x86_64/Banana.exe\", ind_brain=0, no_graphics=True)\n",
    "output_filename = 'checkpoint.pth'\n",
    "collector = banana_collector(goal = 13.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset DDQN agent with parameters:\n",
      "state_size=\t37\n",
      "action_size=\t4\n",
      "hidden_layers=\t [[64, 64, 32], [], []]\n",
      "seed=\t0\n",
      "update_every=\t4\n",
      "batch_size=\t128\n",
      "buffer_size=\t100000\n",
      "learning_rate=\t1.0000e-04\n",
      "tau=\t0.001\n",
      "random_walk=\t [0.75 0.05 0.1  0.1 ]\n",
      "gamma=\t0.990\n",
      "Episode 100\t Composite=-1.33\t Average= 0.55\t Stdev= 1.89\n",
      "Saving checkpoint...\n",
      "Episode 101\t Composite=-1.32\t Average= 0.57\t Stdev= 1.89\n",
      "Saving checkpoint...\n",
      "Episode 105\t Composite=-1.30\t Average= 0.59\t Stdev= 1.90\n",
      "Saving checkpoint...\n",
      "Episode 106\t Composite=-1.29\t Average= 0.62\t Stdev= 1.90\n",
      "Saving checkpoint...\n",
      "Episode 107\t Composite=-1.28\t Average= 0.67\t Stdev= 1.95\n",
      "Saving checkpoint...\n",
      "Episode 108\t Composite=-1.27\t Average= 0.69\t Stdev= 1.96\n",
      "Saving checkpoint...\n",
      "Episode 109\t Composite=-1.16\t Average= 0.77\t Stdev= 1.93\n",
      "Saving checkpoint...\n",
      "Episode 110\t Composite=-1.14\t Average= 0.79\t Stdev= 1.93\n",
      "Saving checkpoint...\n",
      "Episode 111\t Composite=-1.13\t Average= 0.81\t Stdev= 1.94\n",
      "Saving checkpoint...\n",
      "Episode 112\t Composite=-1.11\t Average= 0.83\t Stdev= 1.94\n",
      "Saving checkpoint...\n",
      "Episode 114\t Composite=-1.05\t Average= 0.92\t Stdev= 1.96\n",
      "Saving checkpoint...\n",
      "Episode 118\t Composite=-1.03\t Average= 1.12\t Stdev= 2.15\n",
      "Saving checkpoint...\n",
      "Episode 120\t Composite=-1.01\t Average= 1.14\t Stdev= 2.16\n",
      "Saving checkpoint...\n",
      "Episode 124\t Composite=-0.99\t Average= 1.17\t Stdev= 2.16\n",
      "Saving checkpoint...\n",
      "Episode 132\t Composite=-0.98\t Average= 1.46\t Stdev= 2.44\n",
      "Saving checkpoint...\n",
      "Episode 133\t Composite=-0.95\t Average= 1.50\t Stdev= 2.46\n",
      "Saving checkpoint...\n",
      "Episode 136\t Composite=-0.89\t Average= 1.55\t Stdev= 2.45\n",
      "Saving checkpoint...\n",
      "Episode 138\t Composite=-0.87\t Average= 1.58\t Stdev= 2.45\n",
      "Saving checkpoint...\n",
      "Episode 139\t Composite=-0.84\t Average= 1.60\t Stdev= 2.44\n",
      "Saving checkpoint...\n",
      "Episode 140\t Composite=-0.78\t Average= 1.65\t Stdev= 2.43\n",
      "Saving checkpoint...\n",
      "Episode 141\t Composite=-0.74\t Average= 1.70\t Stdev= 2.44\n",
      "Saving checkpoint...\n",
      "Episode 143\t Composite=-0.73\t Average= 1.72\t Stdev= 2.44\n",
      "Saving checkpoint...\n",
      "Episode 146\t Composite=-0.72\t Average= 1.70\t Stdev= 2.42\n",
      "Saving checkpoint...\n",
      "Episode 147\t Composite=-0.69\t Average= 1.75\t Stdev= 2.43\n",
      "Saving checkpoint...\n",
      "Episode 149\t Composite=-0.67\t Average= 1.76\t Stdev= 2.43\n",
      "Saving checkpoint...\n",
      "Episode 150\t Composite=-0.63\t Average= 1.81\t Stdev= 2.44\n",
      "Saving checkpoint...\n",
      "Episode 152\t Composite=-0.60\t Average= 1.84\t Stdev= 2.44\n",
      "Saving checkpoint...\n",
      "Episode 153\t Composite=-0.57\t Average= 1.86\t Stdev= 2.43\n",
      "Saving checkpoint...\n",
      "Episode 154\t Composite=-0.56\t Average= 1.89\t Stdev= 2.44\n",
      "Saving checkpoint...\n",
      "Episode 157\t Composite=-0.41\t Average= 1.94\t Stdev= 2.35\n",
      "Saving checkpoint...\n",
      "Episode 159\t Composite=-0.41\t Average= 2.21\t Stdev= 2.62\n",
      "Saving checkpoint...\n",
      "Episode 160\t Composite=-0.34\t Average= 2.29\t Stdev= 2.63\n",
      "Saving checkpoint...\n",
      "Episode 161\t Composite=-0.32\t Average= 2.34\t Stdev= 2.67\n",
      "Saving checkpoint...\n",
      "Episode 162\t Composite=-0.28\t Average= 2.41\t Stdev= 2.69\n",
      "Saving checkpoint...\n",
      "Episode 163\t Composite=-0.28\t Average= 2.41\t Stdev= 2.69\n",
      "Saving checkpoint...\n",
      "Episode 164\t Composite=-0.25\t Average= 2.45\t Stdev= 2.71\n",
      "Saving checkpoint...\n",
      "Episode 171\t Composite=-0.21\t Average= 2.67\t Stdev= 2.88\n",
      "Saving checkpoint...\n",
      "Episode 173\t Composite=-0.14\t Average= 2.72\t Stdev= 2.87\n",
      "Saving checkpoint...\n",
      "Episode 175\t Composite=-0.14\t Average= 2.74\t Stdev= 2.88\n",
      "Saving checkpoint...\n",
      "Episode 176\t Composite=-0.08\t Average= 2.79\t Stdev= 2.87\n",
      "Saving checkpoint...\n",
      "Episode 177\t Composite=-0.08\t Average= 2.79\t Stdev= 2.87\n",
      "Saving checkpoint...\n",
      "Episode 178\t Composite=-0.08\t Average= 2.82\t Stdev= 2.89\n",
      "Saving checkpoint...\n",
      "Episode 180\t Composite=-0.06\t Average= 3.01\t Stdev= 3.07\n",
      "Saving checkpoint...\n",
      "Episode 182\t Composite= 0.00\t Average= 3.06\t Stdev= 3.06\n",
      "Saving checkpoint...\n",
      "Episode 183\t Composite= 0.04\t Average= 3.12\t Stdev= 3.08\n",
      "Saving checkpoint...\n",
      "Episode 184\t Composite= 0.13\t Average= 3.18\t Stdev= 3.05\n",
      "Saving checkpoint...\n",
      "Episode 186\t Composite= 0.26\t Average= 3.26\t Stdev= 3.00\n",
      "Saving checkpoint...\n",
      "Episode 188\t Composite= 0.39\t Average= 3.36\t Stdev= 2.97\n",
      "Saving checkpoint...\n",
      "Episode 190\t Composite= 0.40\t Average= 3.45\t Stdev= 3.05\n",
      "Saving checkpoint...\n",
      "Episode 192\t Composite= 0.55\t Average= 3.54\t Stdev= 2.99\n",
      "Saving checkpoint...\n",
      "Episode 194\t Composite= 0.66\t Average= 3.63\t Stdev= 2.97\n",
      "Saving checkpoint...\n",
      "Episode 198\t Composite= 0.78\t Average= 3.77\t Stdev= 2.99\n",
      "Saving checkpoint...\n",
      "Episode 199\t Composite= 0.83\t Average= 3.88\t Stdev= 3.05\n",
      "Saving checkpoint...\n",
      "Episode 200\t Composite= 0.88\t Average= 3.94\t Stdev= 3.06\n",
      "Saving checkpoint...\n",
      "Episode 202\t Composite= 0.91\t Average= 4.17\t Stdev= 3.26\n",
      "Saving checkpoint...\n",
      "Episode 203\t Composite= 1.00\t Average= 4.25\t Stdev= 3.25\n",
      "Saving checkpoint...\n",
      "Episode 204\t Composite= 1.08\t Average= 4.33\t Stdev= 3.25\n",
      "Saving checkpoint...\n",
      "Episode 207\t Composite= 1.13\t Average= 4.38\t Stdev= 3.24\n",
      "Saving checkpoint...\n",
      "Episode 210\t Composite= 1.17\t Average= 4.44\t Stdev= 3.26\n",
      "Saving checkpoint...\n",
      "Episode 211\t Composite= 1.21\t Average= 4.47\t Stdev= 3.26\n",
      "Saving checkpoint...\n",
      "Episode 212\t Composite= 1.25\t Average= 4.57\t Stdev= 3.32\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e81a27ee11ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomposite_score_window\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_score_window\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdev_score_window\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultitrain_tune_LR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_min_exp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_max_exp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\projects\\udacity\\BananaCollectorRL\\banana_collector.py\u001b[0m in \u001b[0;36mmultitrain_tune_LR\u001b[1;34m(self, env, output_filename, lr_min_exp, lr_max_exp, num_train)\u001b[0m\n\u001b[0;32m     24\u001b[0m                                                learning_rate = math.pow(10,lr_exp))\n\u001b[0;32m     25\u001b[0m             \u001b[0mcurrent_filename\u001b[0m      \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"multitrain_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_filename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurrent_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mbest_param\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mlr_exp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\projects\\udacity\\BananaCollectorRL\\banana_collector.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, env, agent, output_filename, n_episodes, window_size, eps_end, eps_decay)\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccumulate_reward\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m                   \u001b[1;31m# select an action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccumulate_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m                 \u001b[0mstate\u001b[0m                       \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0meps\u001b[0m         \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps_end\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_decay\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# decrease epsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\projects\\udacity\\BananaCollectorRL\\ddqn_agent.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;31m# Minimize the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\drlnd\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\drlnd\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_lr, scores, composite_score_window, average_score_window, stdev_score_window = collector.multitrain_tune_LR(env, output_filename, lr_min_exp = -4, lr_max_exp=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the scores\n",
    "\n",
    "fig = plt.figure()\n",
    "ax  = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Train Episode #')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ddqn_agent(state_size=env.state_size, action_size=env.action_size)\n",
    "collector.fullrun(env, agent, output_filename, max_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
